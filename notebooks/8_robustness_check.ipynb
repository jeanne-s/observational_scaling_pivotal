{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'p11 (Python 3.11.5)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'p11 (Python 3.11.5)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'p11 (Python 3.11.5)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "\n",
    "\n",
    "class Bootstrap:\n",
    "\n",
    "    def __init__(self, \n",
    "                 loss,\n",
    "                 lambda_: float = None,\n",
    "                 n_iterations: int = 100, \n",
    "                 random_state: int = None,\n",
    "                 norm_weights: bool = True):\n",
    "        \"\"\"\n",
    "        Initializes the Bootstrap object.\n",
    "\n",
    "        Parameters:\n",
    "        data (array-like): The dataset to bootstrap.\n",
    "        n_iterations (int): Number of bootstrap iterations.\n",
    "        random_state (int): Seed for reproducibility.\n",
    "        \"\"\"\n",
    "\n",
    "        assert loss in ['robust', 'ridge', 'lasso']\n",
    "        self.loss = loss\n",
    "        self.lambda_ = lambda_\n",
    "        self.n_iterations = n_iterations\n",
    "        self.random_state = random_state\n",
    "        self.bootstrap_samples = []\n",
    "        self.norm_weights = norm_weights\n",
    "\n",
    "\n",
    "    def generate_data(self):\n",
    "        \"\"\" /!!!!!!!!!!!!!!\\ CURRENTLY ONLY ON ONE MODEL REMOVED VERSION, NOT TWO MODELS REMOVED\n",
    "        \"\"\"\n",
    "        from utils.data import get_full_post_training_df, regression_raw_benchmarks_difference_cot_naive_get_kwargs, regression_raw_benchmarks_difference_cot_naive, get_weights_from_fit_results\n",
    "        from utils.constants import BBH_SUBTASKS\n",
    "\n",
    "        merged_eval, mmlu_subtasks = get_full_post_training_df()\n",
    "        models_list = merged_eval['Model'].unique().tolist()\n",
    "\n",
    "        # The first 2 models are Llama2, used for the equiv scale so they can't be removed\n",
    "        lists_with_one_model_removed = [models_list[:i] + models_list[i+1:] for i in range(2, len(models_list))] \n",
    "        print('Number of lists with one model removed', len(lists_with_one_model_removed))\n",
    "\n",
    "        lists_with_two_models_removed = [models_list[:i] + models_list[i+1:j] + models_list[j+1:] for i in range(2, len(models_list)) for j in range(i+1, len(models_list))]\n",
    "        print('Number of lists with two models removed', len(lists_with_two_models_removed))\n",
    "\n",
    "        all_fit_results = pd.DataFrame()\n",
    "        for bbh_subtask in BBH_SUBTASKS:\n",
    "            regression_kwargs = regression_raw_benchmarks_difference_cot_naive_get_kwargs(bbh_task=bbh_subtask,\n",
    "                                                                                          other_subtasks=mmlu_subtasks,\n",
    "                                                                                          loss=self.loss,\n",
    "                                                                                          lambda_=self.lambda_)\n",
    "            \n",
    "            for s, model_sublist in enumerate(lists_with_one_model_removed):\n",
    "                sub_merged_eval = merged_eval[merged_eval['Model'].isin(model_sublist)]\n",
    "                fit_results = regression_raw_benchmarks_difference_cot_naive(base_llm_eval_with_post_training=sub_merged_eval,\n",
    "                                                                             regression_kwargs=regression_kwargs)\n",
    "                weights = get_weights_from_fit_results(fit_results=fit_results,\n",
    "                                                       regression_kwargs=regression_kwargs,\n",
    "                                                       norm_weights=self.norm_weights)\n",
    "\n",
    "                temp_df = {\n",
    "                    'subtask': bbh_subtask,\n",
    "                    'sublist_id': s\n",
    "                }\n",
    "                for i, weight in enumerate(weights):\n",
    "                    temp_df[regression_kwargs['metric_list'][i]] = weight\n",
    "                all_fit_results = pd.concat([all_fit_results, pd.DataFrame(temp_df)], ignore_index=True)\n",
    "\n",
    "        return all_fit_results\n",
    "\n",
    "\n",
    "    def generate_samples(self):\n",
    "        \"\"\"\n",
    "        Generates bootstrap samples.\n",
    "        \"\"\"\n",
    "        from utils.constants import BBH_SUBTASKS\n",
    "        all_fit_results = self.generate_data()\n",
    "        n_sublists = len(all_fit_results['sublist_id'].unique())\n",
    "\n",
    "        np.random.seed(self.random_state)\n",
    "        for i in range(self.n_iterations):\n",
    "            for bbh_subtask in BBH_SUBTASKS:\n",
    "                subtask_data = all_fit_results[(all_fit_results['subtask'] == bbh_subtask) & (all_fit_results['sublist_id'] == i%n_sublists)]\n",
    "                float_columns = subtask_data.select_dtypes(include='float64')\n",
    "                data_array = float_columns.to_numpy()\n",
    "            \n",
    "            sample = resample(data_array, replace=True)\n",
    "            self.bootstrap_samples.append(sample)\n",
    "\n",
    "\n",
    "    def compute_statistic(self, statistic_func):\n",
    "        \"\"\"\n",
    "        Computes a statistic over the bootstrap samples.\n",
    "\n",
    "        Parameters:\n",
    "        statistic_func (function): Function to compute the statistic.\n",
    "\n",
    "        Returns:\n",
    "        list: List of statistic values for each bootstrap sample.\n",
    "        \"\"\"\n",
    "        if not self.bootstrap_samples:\n",
    "            self.generate_samples()\n",
    "        \n",
    "        results = [statistic_func(sample) for sample in self.bootstrap_samples]\n",
    "        return results\n",
    "\n",
    "\n",
    "    def summary(self, results):\n",
    "        \"\"\"\n",
    "        Summarizes the bootstrap results.\n",
    "\n",
    "        Parameters:\n",
    "        stats (list): List of statistic values.\n",
    "\n",
    "        Returns:\n",
    "        dict: Dictionary with mean, standard error, and confidence interval.\n",
    "        \"\"\"\n",
    "        stats = [result[0] for result in results]\n",
    "        zero_in_ci_mask = [result[1] for result in results]\n",
    "        masked = [np.ma.masked_where(zero_in_ci_mask[i], stats[i]) for i in range(0, len(stats))]\n",
    "\n",
    "        # Count the number of times 0 is in the confidence interval\n",
    "        false_counts = np.sum(~zero_in_ci_mask, axis=0)\n",
    "\n",
    "        mean_stat = np.mean(masked, axis=0)\n",
    "        std_error = np.std(masked, axis=0)\n",
    "        confidence_interval = np.percentile(masked, [2.5, 97.5], axis=0)\n",
    "        \n",
    "        return {\n",
    "            'stats': masked,\n",
    "            'mean': mean_stat,\n",
    "            'std_error': std_error,\n",
    "            'confidence_interval': confidence_interval,\n",
    "            'false_counts': false_counts\n",
    "        }\n",
    "    \n",
    "\n",
    "def average_weights(sample):\n",
    "    mean_sample = np.mean(sample, axis=1)\n",
    "    confidence_interval_sample = np.percentile(sample, [2.5, 97.5], axis=1)\n",
    "    zero_in_ci = np.sign(confidence_interval_sample[0]*confidence_interval_sample[1])\n",
    "    zero_in_ci_mask = zero_in_ci < 0 # If 0 is contained in the confidence interval, the value will be removed from the mean calculus\n",
    "    return mean_sample, zero_in_ci_mask\n",
    "\n",
    "\n",
    "def plot_raw_benchmarks_weights(weights: np.ndarray,\n",
    "                                metric_list: list,\n",
    "                                vmax=1\n",
    "):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(18,1))\n",
    "    sns.heatmap(weights.reshape((1,31)), annot=True, fmt='.2f', \n",
    "                cmap='coolwarm', vmin=-vmax, vmax=vmax, \n",
    "                ax=ax, annot_kws={\"fontsize\":8})\n",
    "        \n",
    "    ax.set_xticklabels(metric_list, rotation=90)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap = Bootstrap(loss='ridge',\n",
    "                      lambda_=1e-1,\n",
    "                      n_iterations=1,\n",
    "                      random_state=42)\n",
    "\n",
    "stats = bootstrap.compute_statistic(average_weights)\n",
    "\n",
    "summary = bootstrap.summary(stats)\n",
    "print('Number of times 0 is in the confidence interval', summary['false_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'p11 (Python 3.11.5)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from utils.data import get_full_post_training_df\n",
    "merged_eval, mmlu_subtasks = get_full_post_training_df()\n",
    "\n",
    "#plot_raw_benchmarks_weights(weights=summary['mean'],\n",
    "#                            metric_list=['ARC-C', 'HellaSwag', 'Winograd', 'TruthfulQA', 'GSM8K', 'XWinograd', 'HumanEval']+mmlu_subtasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'p11 (Python 3.11.5)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "merged_eval['Model'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
